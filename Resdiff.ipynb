{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Latent diffusion attempt\n"
      ],
      "metadata": {
        "id": "wUigOX0zQOgx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SXg64As-QLzf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4873eff2-dd72-4bb9-a219-9cab3f9baa06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/53.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.8/53.8 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install einops -q\n",
        "!pip install lpips -q\n",
        "\n",
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from einops import rearrange\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import MNIST\n",
        "import tqdm\n",
        "from tqdm.notebook import trange, tqdm\n",
        "from torch.optim.lr_scheduler import MultiplicativeLR, LambdaLR\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lin1 = torch.nn.Linear(64, 32)\n",
        "lin2 = torch.nn.Linear(32, 16)\n",
        "x = torch.rand((16,4096, 64))\n",
        "lin2(lin1(x)).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sdCpgyojdI6B",
        "outputId": "e8b3b878-1654-4628-a824-5906e6b95943"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([16, 4096, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "General diffusion utils"
      ],
      "metadata": {
        "id": "g31DIcVx_yc2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "beta_min = 1e-4\n",
        "beta_max = 0.02\n",
        "\n",
        "num_steps = 500\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "betas = np.linspace(beta_min, beta_max, num_steps)\n",
        "betas = torch.from_numpy(betas).float().to(DEVICE)\n",
        "\n",
        "alphas = 1.0 - betas\n",
        "alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
        "alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0)\n",
        "\n",
        "sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n",
        "sqrt_one_minus_alphas_cumprod = torch.sqrt(1.0 - alphas_cumprod)\n",
        "log_one_minus_alphas_cumprod = torch.log(1.0 - alphas_cumprod)\n",
        "sqrt_recip_alphas_cumprod = torch.sqrt(1.0 / alphas_cumprod)\n",
        "sqrt_recipm1_alphas_cumprod = torch.sqrt(1.0 / alphas_cumprod - 1)\n",
        "\n",
        "posterior_variance = betas * (1.0 - alphas_cumprod_prev) / (1.0 - alphas_cumprod)\n",
        "posterior_log_variance_clipped = torch.log(posterior_variance.clamp(min=1e-20))\n",
        "posterior_mean_coef1 = betas * torch.sqrt(alphas_cumprod_prev) / (1.0 - alphas_cumprod)\n",
        "posterior_mean_coef2 = (\n",
        "    (1.0 - alphas_cumprod_prev) * torch.sqrt(alphas) / (1.0 - alphas_cumprod))"
      ],
      "metadata": {
        "id": "T-cLg-lu_x8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loss function definition:\n",
        "\n",
        "$ J := \\mathbb{E}_{t\\in (0, T), x_0 \\sim p_0(x_0), \\epsilon \\sim \\mathcal N(0,I)}\\left[ \\ \\Vert  \\epsilon - \\epsilon_{\\theta}(x_t, t) \\Vert^2  \\ \\right] $\n",
        "\n",
        "Considering $x_{residuals} = x_{HR}- up(x_{LR})$ as $x_t$"
      ],
      "metadata": {
        "id": "n9iJFJGEIGAx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_fn_cond(model, x, y):\n",
        "    \"\"\"The loss function for training score-based generative models.\n",
        "\n",
        "    Args:\n",
        "      model: A PyTorch model instance that represents a\n",
        "        time-dependent score-based model.\n",
        "      x: A mini-batch of LR training data.\n",
        "      y: Corresponding HR result.\n",
        "    \"\"\"\n",
        "    # Estimate Residuals between LR-HR\n",
        "    residuals = torch.subtract(x, y)\n",
        "\n",
        "    # Sample time uniformly in 0, num_steps\n",
        "    random_t = torch.randint(low=0, high=num_steps, size=(x.shape[0],), device=DEVICE)\n",
        "    z = torch.randn_like(x)  # get normally distributed noise\n",
        "\n",
        "    perturbed_x = (\n",
        "        sqrt_alphas_cumprod[random_t][..., None, None, None] * residuals \\\n",
        "        + sqrt_one_minus_alphas_cumprod[random_t][..., None, None, None] * z\n",
        "    )\n",
        "    eps = model(perturbed_x, random_t)\n",
        "    loss = torch.mean((z - eps) ** 2)\n",
        "    return loss"
      ],
      "metadata": {
        "id": "-WCPCJy6GlTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Defining Autoencoder\n"
      ],
      "metadata": {
        "id": "qs-yOEpoSc6n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AEmodel(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, input_channels, channels=[8,16,32]):\n",
        "        \"\"\"Args:\n",
        "          channels: The number of channels for feature maps of each resolution.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # Gaussian random feature embedding layer for time\n",
        "        # Encoding layers where the resolution decreases\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(input_channels, channels[0], 3, stride=1, bias=True),\n",
        "            nn.BatchNorm2d(channels[0]),\n",
        "            nn.SiLU(),\n",
        "            nn.Conv2d(channels[0], channels[1], 3, stride=2, bias=True),\n",
        "            nn.BatchNorm2d(channels[1]),\n",
        "            nn.SiLU(),\n",
        "            nn.Conv2d(channels[1], channels[2], 3, stride=1, bias=True),\n",
        "            nn.BatchNorm2d(channels[2]))\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(channels[2], channels[1], 3, stride=1, bias=True),\n",
        "            nn.BatchNorm2d(channels[1]),\n",
        "            nn.SiLU(),\n",
        "            nn.ConvTranspose2d(channels[1], channels[0], 3, stride=2, bias=True, output_padding=1),\n",
        "            nn.BatchNorm2d(channels[0]),\n",
        "            nn.SiLU(),\n",
        "            nn.ConvTranspose2d(channels[0], input_channels, 3, stride=1, bias=True),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.decoder(self.encoder(x))"
      ],
      "metadata": {
        "id": "i-WfhmmsQcen"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Autoencoder with time embdedding"
      ],
      "metadata": {
        "id": "E4WdWUfvSgaO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining time embedding. (not actually clear why we may need a time embdedded autoencoder)"
      ],
      "metadata": {
        "id": "ZaWO6jjBStlz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GaussianFourierProjection(nn.Module):\n",
        "    \"\"\"Gaussian random features for encoding time steps.\"\"\"\n",
        "\n",
        "    def __init__(self, embed_dim, scale=30.0):\n",
        "        super().__init__()\n",
        "        # Randomly sample weights (frequencies) during initialization.\n",
        "        # These weights (frequencies) are fixed during optimization and are not trainable.\n",
        "        self.W = nn.Parameter(torch.randn(embed_dim // 2) * scale, requires_grad=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\" Args:\n",
        "            x: torch.Tensor of dim 1 (e.g. torch.Tensor([12]))\n",
        "        \"\"\"\n",
        "        x_proj = x[:, None] * self.W[None, :] * 2 * np.pi\n",
        "        return torch.cat([torch.sin(x_proj), torch.cos(x_proj)], dim=-1)\n",
        "\n",
        "\n",
        "class Dense(nn.Module):\n",
        "    \"\"\"A fully connected layer that reshapes outputs to feature maps.\n",
        "    Allow time repr to input additively from the side of a convolution layer.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.dense(x)[..., None, None]\n",
        "        # this broadcast the 2d tensor to 4d, add the same value across space.\n",
        "\n",
        "\n",
        "class AEmodel(nn.Module):\n",
        "\n",
        "    def __init__(self, input_channels, channels=[4, 8, 16], embed_dim=256):\n",
        "        \"\"\"Args:\n",
        "          channels: The number of channels for feature maps of each resolution.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        pass\n",
        "\n",
        "    def forward(self, x):\n",
        "        raise NotImplementedError\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_u5rfiezSt3b",
        "outputId": "a8bc4785-a4d8-4e54-f7fd-0de6cb36cb23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedded dim: torch.Size([1, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Autoencoder Training\n",
        "Decide whether to have a pretrained autoencoder for class domain (climate variable)\n"
      ],
      "metadata": {
        "id": "NYw44tdDlc7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "se = SimpleEncoder(1)\n",
        "x = torch.rand((4,1,32,32))\n",
        "pytorch_total_params = sum(p.numel() for p in se.parameters() if p.requires_grad)\n",
        "print(se(x).shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LeydnHNKiTz",
        "outputId": "651bd086-1694-4b21-aa0b-14997d5ad842"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 32, 8, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Attention modules\n"
      ],
      "metadata": {
        "id": "fDPp9VXvK5Oo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Channel Cross-Attention approach\n",
        "Requires a joint latent representation including the climate data along with its high frequency representation to match H·W dimension at each downsampling stage.\n",
        "\n"
      ],
      "metadata": {
        "id": "OjZSKId5ryZ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, nchannels, ndepth):\n",
        "        super().__init__()\n",
        "        self.ndepth = ndepth\n",
        "        self.act = nn.SiLU()\n",
        "        self.res_conv_blocks = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Conv2d(nchannels, nchannels, 3, stride=1, padding=\"same\", bias=True),\n",
        "                nn.BatchNorm2d(nchannels),\n",
        "                nn.SiLU(),\n",
        "                nn.Conv2d(nchannels, nchannels, 3, stride=1, padding=\"same\", bias=True),\n",
        "                nn.BatchNorm2d(nchannels),\n",
        "                )\n",
        "            for _ in range(self.ndepth)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        for conv_block in self.res_conv_blocks:\n",
        "            x = self.act(conv_block(x) + x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ResImgEncoder(nn.Module):\n",
        "    \"\"\"Simple Image encoder to be used in the Q, K, V transcription\"\"\"\n",
        "    def __init__(self, nchannels, ndepth=1, nheads=1, prehead=\"same\"):\n",
        "        \"\"\" Args:\n",
        "            nchannels: (int) input x_feature channel.\n",
        "            ndepth: (int) amount of ResBlock with skip connection.\n",
        "            nheads: (int) factor to enlarge channel dimension before crossattention.\n",
        "            prehead: (str) accepted ('preupsample','postupsample', 'same')\n",
        "        \"\"\"\n",
        "        super(ResImgEncoder, self).__init__()\n",
        "\n",
        "        self.outchannels = nchannels * nheads\n",
        "\n",
        "        if prehead == \"preupsample\":\n",
        "            # move first into outchannel dim via 1x1 convblock and go on with\n",
        "            # ResBlocks at outchannels\n",
        "            in_c, mid_c, out_c = nchannels, self.outchannels, self.outchannels\n",
        "        elif prehead == \"postupsample\":\n",
        "            # enlarge channel dimension at the end\n",
        "            in_c, mid_c, out_c = nchannels, nchannels, self.outchannels\n",
        "        elif prehead == \"same\":\n",
        "            # Preserve same dimension\n",
        "            in_c, mid_c, out_c = nchannels, nchannels, nchannels\n",
        "        else:\n",
        "            raise ValueError(\"prehead values accepted : ('preupsample','postupsample', 'same')\")\n",
        "\n",
        "        self.conv1pre = nn.Conv2d(in_c, mid_c, 1, stride=1, bias=True)\n",
        "        self.res_conv_blocks = ResBlock(mid_c, ndepth)\n",
        "        self.conv1post = nn.Conv2d(mid_c, out_c, 1, stride=1, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1pre(x)\n",
        "        x = self.res_conv_blocks(x)\n",
        "        x = self.conv1post(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ChannelAttention(nn.Module):\n",
        "    \"\"\"Simple Cross-attention High Frequency Guided Module\"\"\"\n",
        "    def __init__(self, nchannels, ndepth=1, nheads=1, prehead=\"same\"):\n",
        "        super(ChannelAttention, self).__init__()\n",
        "        self.outchannels = nchannels if prehead==\"same\" else nchannels * nheads\n",
        "        # Learnable d parameter\n",
        "        self.d = torch.nn.Parameter(\n",
        "                torch.abs(torch.rand((1)) + torch.randint(10//2, 10*2, (1,))\n",
        "                ))\n",
        "        self.d.requires_grad=True\n",
        "        # Convolutional encoder to process Q, K, V\n",
        "        self.Q_feature_encoder = ResImgEncoder(nchannels, ndepth, nheads, prehead)\n",
        "        self.K_feature_encoder = ResImgEncoder(nchannels, ndepth, nheads, prehead)\n",
        "        self.V_feature_encoder = ResImgEncoder(nchannels, ndepth, nheads, prehead)\n",
        "        self.crossattn_encoder = ResImgEncoder(1, ndepth, nheads=1)\n",
        "        self.conv1 = nn.Conv2d(self.outchannels, nchannels, kernel_size=1, stride=1, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\" Args:\n",
        "        x: (torch.Tensor) LR image of dim (b, c, h, w)\n",
        "        \"\"\"\n",
        "        b, _, h, w = x.shape\n",
        "        c = self.outchannels\n",
        "        # Generate Q,K,V via img encoder. Outshape:(batch, h*w, in_channels*nheads)\n",
        "        Q = self.Q_feature_encoder(x).view(b, h*w, c)\n",
        "        K = self.K_feature_encoder(x).view(b, h*w, c)\n",
        "        V = self.V_feature_encoder(x).view(b, h*w, c)\n",
        "        # Channel-wise product\n",
        "        scoremats = torch.einsum(\"bjk,bjc->bkc\", Q, K).view(b, 1, c, c)\n",
        "        scoremats = self.crossattn_encoder(scoremats).view(b, c, c)\n",
        "        attnmats  = F.softmax(scoremats / torch.sqrt(self.d), dim=-1)\n",
        "        # Back into batch size\n",
        "        ctx_vecs  = torch.einsum(\"bkc,bij->bic\", attnmats, V).view(b, c, h, w)\n",
        "        # 1x1 conv to convert into (batch, in_channel, h , w)\n",
        "        ctx_vecs  = self.conv1(ctx_vecs)\n",
        "        return  ctx_vecs + x\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, nchannels, nhidden=128, ndepth=1, nheads=1, prehead=\"same\"):\n",
        "        super().__init__()\n",
        "        self.ca = ChannelAttention(nchannels, ndepth, nheads, prehead)\n",
        "        self.norm1 = nn.InstanceNorm2d(nchannels)\n",
        "        self.norm2 = nn.InstanceNorm2d(nchannels)\n",
        "        self.mlp   = nn.Sequential(\n",
        "            nn.Linear(nchannels, nhidden),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(nhidden, nchannels),\n",
        "        )\n",
        "        #self.conv = nn.Conv2d(nchannels, nchannels, 3, stride=1, padding=\"same\",\n",
        "        #                      bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, h, w = x.shape\n",
        "        x_norm = self.norm1(x)\n",
        "        x = self.ca(x_norm) + x\n",
        "        x_norm = self.norm2(x).view(b, h*w, c)\n",
        "        x = self.mlp(x_norm).view(b, c, h, w) + x\n",
        "        return x\n",
        "\n",
        "\n",
        "class HF_Encoder(nn.Module):\n",
        "    def __init__(self, nblocks, nchannels, nhidden, ndepth=1, nheads=1,\n",
        "                 prehead=\"same\"):\n",
        "        super().__init__()\n",
        "        self.transformer = nn.ModuleList([\n",
        "            TransformerBlock(nchannels, nhidden, ndepth, nheads, prehead)\n",
        "            for _ in range(nblocks)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x):\n",
        "        for encode_layer in self.transformer:\n",
        "            x = encode_layer(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import time\n",
        "    t0 = time.time()\n",
        "    nchannels = 64\n",
        "    x = torch.rand((1, nchannels, 64, 64))\n",
        "    net = HF_Encoder(nblocks=1, nchannels=nchannels, nhidden=64,\n",
        "                     ndepth=4, nheads=1, prehead=\"same\")\n",
        "\n",
        "    print(f\"Learnable params: {sum(p.numel() for p in net.parameters() if p.requires_grad)}\")\n",
        "\n",
        "    t1 = time.time()\n",
        "    print(net(x).shape, t1-t0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2nPd76sryqJ",
        "outputId": "3b7cb5c6-212b-467e-ca45-e2fa7f32c616"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learnable params: 926821\n",
            "torch.Size([1, 64, 64, 64]) 0.022253990173339844\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sequence based attention implementation"
      ],
      "metadata": {
        "id": "dC6dEXoQUo3L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ImgEncoder(nn.Module):\n",
        "    \"\"\"Simple Image encoder to be used in the Q, K, V transcription\"\"\"\n",
        "    def __init__(self, input_channels, channels=[8,16,32]):\n",
        "        super(ImgEncoder, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(input_channels, channels[0], 3, stride=1, padding=\"same\", bias=True),\n",
        "            nn.BatchNorm2d(channels[0]),\n",
        "            nn.MaxPool2d(2, stride=2),\n",
        "            nn.SiLU(),\n",
        "            nn.Conv2d(channels[0], channels[1], 3, stride=1, padding=\"same\", bias=True),\n",
        "            nn.BatchNorm2d(channels[1]),\n",
        "            nn.MaxPool2d(2, stride=2),\n",
        "            nn.SiLU(),\n",
        "            nn.Conv2d(channels[1], channels[2], 3, stride=1, padding=\"same\", bias=True),\n",
        "            nn.BatchNorm2d(channels[2])\n",
        "            )\n",
        "    def forward(self, x):\n",
        "        \"\"\" outdim: [b, channels[-1], H/4, W/4] \"\"\"\n",
        "        x = self.encoder(x)\n",
        "        return x # reduced H_{out} = H/4\n",
        "\n",
        "\n",
        "class CrossAttention(nn.Module):\n",
        "    \"\"\"Simple Cross-attention High Frequency Guided Module\"\"\"\n",
        "    def __init__(self, in_channel, feature_dim, hidden_dim=256, out_dim=256):\n",
        "        super(CrossAttention, self).__init__()\n",
        "        self.hidden_dim  = hidden_dim\n",
        "        self.h, self.w = feature_dim\n",
        "\n",
        "        # Learnable d parameter\n",
        "        self.d = torch.nn.Parameter(\n",
        "                torch.abs(torch.rand((1)) + torch.randint(10//2, 10*2, (1,))\n",
        "                ))\n",
        "        self.d.requires_grad=True\n",
        "\n",
        "        # Convolutional encoder to process Q, K, V\n",
        "        self.HF_encoder   = SimpleEncoder(in_channel)\n",
        "        self.feat_encoder = SimpleEncoder(in_channel)\n",
        "        self.linear = nn.Sequential(\n",
        "            nn.Linear(self.h//4*self.w//4, hidden_dim, bias=True),\n",
        "            nn.GELU,\n",
        "            nn.Linear(hidden_dim, out_dim)\n",
        "            )\n",
        "\n",
        "    def forward(self, x, orig_hf):\n",
        "        \"\"\" Args:\n",
        "        x: (torch.Tensor) LR image of dim (b, c, h, w)\n",
        "        orig_hf: (torch.Tensor) original High-Frequency map of dim (b, 1, h, w)\n",
        "        \"\"\"\n",
        "        b, _, h, w = x.shape\n",
        "        # tokens: with shape (batch, h*w, hdim)\n",
        "        Q = self.convHF(orig_hf).view(b, h*w, self.hidden_dim)\n",
        "        Q = self.linear(Q).view(b, )\n",
        "\n",
        "        K = self.convK(x).view(b, h*w, self.hidden_dim)\n",
        "        V = self.convV(x).view(b, h*w, self.hidden_dim)\n",
        "\n",
        "        #scoremats = torch.einsum(\"BTH,BSH->BTS\", Q, K)  # inner product of Q and K, a tensor\n",
        "        # Channel-wise product\n",
        "        scoremats = torch.einsum(\"bjk,bjc->bkc\", Q, K)\\\n",
        "                    .view(b, 1, self.hidden_dim, self.hidden_dim)\n",
        "        scoremats = self.convA(scoremats)\\\n",
        "                    .view(b, self.hidden_dim, self.hidden_dim)\n",
        "        attnmats  = F.softmax(scoremats / torch.sqrt(self.d), dim=-1)\n",
        "        #ctx_vecs  = torch.einsum(\"BTS,BSH->BTH\", attnmats, V)\n",
        "        # To (batch, hdim, h, w)\n",
        "        ctx_vecs  = torch.einsum(\"bkc,bij->bic\", attnmats, V)\\\n",
        "                    .view(b, self.hidden_dim, h, w)\n",
        "        # 1k conv to (batch, in_channel, h , w)\n",
        "        ctx_vecs  = self.conv1(ctx_vecs)\n",
        "        return x + ctx_vecs\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    \"\"\"The transformer block that combines self-attn, cross-attn and feed forward neural net\"\"\"\n",
        "\n",
        "    def __init__(self, in_channel, hidden_dim):# context_dim):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.attn_self = CrossAttention(in_channel, hidden_dim)\n",
        "        #self.attn_cross = CrossAttention(hidden_dim, hidden_dim, context_dim)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(hidden_dim)\n",
        "        self.norm2 = nn.LayerNorm(hidden_dim)\n",
        "        self.norm3 = nn.LayerNorm(hidden_dim)\n",
        "        # implement a 2 layer MLP with K*hidden_dim hidden units, and nn.GeLU nonlinearity #######\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, 3 * hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(3 * hidden_dim, hidden_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, orig_hf):#, context=None):\n",
        "        \"\"\" Args:\n",
        "        x: (torch.Tensor) LR image of dim (b, c, h, w)\n",
        "        orig_hf: (torch.Tensor) original High-Frequency map of dim (b, 1, h, w)\n",
        "        \"\"\"\n",
        "        # Notice the + x as residue connections\n",
        "        x = self.attn_self(self.norm1(x), orig_hf) + x\n",
        "        # Notice the + x as residue connections\n",
        "        #x = self.attn_cross(self.norm2(x), context=context) + x\n",
        "        # Notice the + x as residue connections\n",
        "        x = self.ffn(self.norm3(x)) + x\n",
        "        return x\n",
        "\n",
        "\n",
        "class SpatialTransformer(nn.Module):\n",
        "    def __init__(self, in_channel, hidden_dim):\n",
        "        super(SpatialTransformer, self).__init__()\n",
        "        self.hf_encoder  = ImgEncoder(1, channels=[8,16,32])\n",
        "        self.img_encoder = ImgEncoder(in_channel, channels=[8,16,32])\n",
        "\n",
        "        self.transformer = TransformerBlock(in_channel, hidden_dim)\n",
        "\n",
        "    def forward(self, x, orig_hf):# context=None):\n",
        "        b, c, h, w = x.shape\n",
        "        x_in = x\n",
        "        # Combine the spatial dimensions and move the channel dimen to the end\n",
        "        x = rearrange(x, \"b c h w->b (h w) c\")\n",
        "        # Apply the sequence transformer\n",
        "        x = self.transformer(x, orig_hf)\n",
        "        # Reverse the process\n",
        "        x = rearrange(x, \"b (h w) c -> b c h w\", h=h, w=w)\n",
        "        # Residue\n",
        "        return x + x_in"
      ],
      "metadata": {
        "id": "gbtdYm93SaT0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fully working Cross Attention mechanism with images"
      ],
      "metadata": {
        "id": "JFsGr2M1UlBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CrossAttention(nn.Module):\n",
        "    \"\"\"Simple Cross-attention High Frequency Guided Module\"\"\"\n",
        "    def __init__(self, in_channel, hidden_dim=128):\n",
        "        super(CrossAttention, self).__init__()\n",
        "        self.hidden_dim  = hidden_dim\n",
        "        # Learnable d parameter\n",
        "        self.d = torch.nn.Parameter(\n",
        "                torch.abs(torch.rand((1)) + torch.randint(10//2, 10*2, (1,))\n",
        "                ))\n",
        "        self.d.requires_grad=True\n",
        "\n",
        "        # Convolutional encoder to process Q, K, V\n",
        "        self.convHF = nn.Conv2d(1, hidden_dim, 3, 1, \"same\", bias=False)\n",
        "        self.convK  = nn.Conv2d(in_channel, hidden_dim, 3, 1, \"same\", bias=False)\n",
        "        self.convV  = nn.Conv2d(in_channel, hidden_dim, 3, 1, \"same\", bias=False)\n",
        "        self.convA  = nn.Conv2d(1, 1, kernel_size=3, stride=1, padding=\"same\", bias=False)\n",
        "        self.conv1  = nn.Conv2d(hidden_dim, in_channel, 1, bias=False)\n",
        "\n",
        "    def forward(self, x, orig_hf):\n",
        "        \"\"\" Args:\n",
        "        x: (torch.Tensor) LR image of dim (b, c, h, w)\n",
        "        orig_hf: (torch.Tensor) original High-Frequency map of dim (b, 1, h, w)\n",
        "        \"\"\"\n",
        "        b, _, h, w = x.shape\n",
        "        # tokens: with shape (batch, h*w, hdim)\n",
        "        Q = self.convHF(orig_hf).view(b, h*w, self.hidden_dim)\n",
        "        K = self.convK(x).view(b, h*w, self.hidden_dim)\n",
        "        V = self.convV(x).view(b, h*w, self.hidden_dim)\n",
        "\n",
        "\n",
        "        #scoremats = torch.einsum(\"BTH,BSH->BTS\", Q, K)  # inner product of Q and K, a tensor\n",
        "        # Channel-wise product\n",
        "        scoremats = torch.einsum(\"bjk,bjc->bkc\", Q, K)\\\n",
        "                    .view(b, 1, self.hidden_dim, self.hidden_dim)\n",
        "        scoremats = self.convA(scoremats)\\\n",
        "                    .view(b, self.hidden_dim, self.hidden_dim)\n",
        "        attnmats  = F.softmax(scoremats / torch.sqrt(self.d), dim=-1)\n",
        "        #ctx_vecs  = torch.einsum(\"BTS,BSH->BTH\", attnmats, V)\n",
        "        # To (batch, hdim, h, w)\n",
        "        ctx_vecs  = torch.einsum(\"bkc,bij->bic\", attnmats, V)\\\n",
        "                    .view(b, self.hidden_dim, h, w)\n",
        "        # 1k conv to (batch, in_channel, h , w)\n",
        "        ctx_vecs  = self.conv1(ctx_vecs)\n",
        "        return x + ctx_vecs\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    \"\"\"The transformer block that combines self-attn, cross-attn and feed forward neural net\"\"\"\n",
        "\n",
        "    def __init__(self, in_channel, hidden_dim):# context_dim):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.attn_self = CrossAttention(in_channel, hidden_dim)\n",
        "        #self.attn_cross = CrossAttention(hidden_dim, hidden_dim, context_dim)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(hidden_dim)\n",
        "        self.norm2 = nn.LayerNorm(hidden_dim)\n",
        "        self.norm3 = nn.LayerNorm(hidden_dim)\n",
        "        # implement a 2 layer MLP with K*hidden_dim hidden units, and nn.GeLU nonlinearity #######\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, 3 * hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(3 * hidden_dim, hidden_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, orig_hf):#, context=None):\n",
        "        \"\"\" Args:\n",
        "        x: (torch.Tensor) LR image of dim (b, c, h, w)\n",
        "        orig_hf: (torch.Tensor) original High-Frequency map of dim (b, 1, h, w)\n",
        "        \"\"\"\n",
        "        # Notice the + x as residue connections\n",
        "        x = self.attn_self(self.norm1(x), orig_hf) + x\n",
        "        # Notice the + x as residue connections\n",
        "        #x = self.attn_cross(self.norm2(x), context=context) + x\n",
        "        # Notice the + x as residue connections\n",
        "        x = self.ffn(self.norm3(x)) + x\n",
        "        return x\n",
        "\n",
        "\n",
        "class SpatialTransformer(nn.Module):\n",
        "    def __init__(self, in_channel, hidden_dim):\n",
        "        super(SpatialTransformer, self).__init__()\n",
        "        self.transformer = TransformerBlock(in_channel, hidden_dim)\n",
        "\n",
        "    def forward(self, x, orig_hf):# context=None):\n",
        "        b, c, h, w = x.shape\n",
        "        x_in = x\n",
        "        # Combine the spatial dimensions and move the channel dimen to the end\n",
        "        x = rearrange(x, \"b c h w->b (h w) c\")\n",
        "        # Apply the sequence transformer\n",
        "        x = self.transformer(x, orig_hf)\n",
        "        # Reverse the process\n",
        "        x = rearrange(x, \"b (h w) c -> b c h w\", h=h, w=w)\n",
        "        # Residue\n",
        "        return x + x_in\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "    in_channel = 1\n",
        "    hidden_dim = 64\n",
        "    ca = CrossAttention(in_channel, hidden_dim)\n",
        "    x = torch.rand((4, in_channel, 64, 64))\n",
        "    orig_hf = torch.rand((4, 1, 64, 64))\n",
        "\n",
        "    out = ca(x,orig_hf)\n",
        "    conv1 = nn.Conv2d(hidden_dim, 1, 1, bias=False)\n",
        "\n",
        "    #tb = TransformerBlock(in_channel, hidden_dim)\n",
        "    #norm = nn.LayerNorm(hidden_dim)\n",
        "    #print(norm(x).shape)\n",
        "    print(\"CrossAtt output\", out.shape)\n",
        "    #print(\"Transfb output:\", tb(x, orig_hf).shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zoztwVc2K7Rw",
        "outputId": "f10008e1-331a-44f7-f0ec-c591de7cf9a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CrossAtt output torch.Size([4, 1, 64, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Orig implementation"
      ],
      "metadata": {
        "id": "pJvPXslrU3jG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CrossAttention(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        embed_dim,\n",
        "        hidden_dim,\n",
        "        context_dim=None,\n",
        "        num_heads=1,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Note: For simplicity reason, we just implemented 1-head attention.\n",
        "        Feel free to implement multi-head attention! with fancy tensor manipulations.\n",
        "        \"\"\"\n",
        "        super(CrossAttention, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.context_dim = context_dim\n",
        "        self.embed_dim = embed_dim\n",
        "        self.query = nn.Linear(hidden_dim, embed_dim, bias=False)\n",
        "        if context_dim is None:\n",
        "            self.self_attn = True\n",
        "            self.key = nn.Linear(hidden_dim, embed_dim, bias=False)  ###########\n",
        "            self.value = nn.Linear(hidden_dim, hidden_dim, bias=False)  ############\n",
        "        else:\n",
        "            self.self_attn = False\n",
        "            self.key = nn.Linear(context_dim, embed_dim, bias=False)  #############\n",
        "            self.value = nn.Linear(context_dim, hidden_dim, bias=False)  ############\n",
        "\n",
        "    def forward(self, tokens, context=None):\n",
        "        # tokens: with shape [batch, sequence_len, hidden_dim]\n",
        "        # context: with shape [batch, contex_seq_len, context_dim]\n",
        "        if self.self_attn:\n",
        "            Q = self.query(tokens)\n",
        "            K = self.key(tokens)\n",
        "            V = self.value(tokens)\n",
        "        else:\n",
        "            # implement Q, K, V for the Cross attention\n",
        "            Q = self.query(tokens)\n",
        "            K = self.key(context)\n",
        "            V = self.value(context)\n",
        "        # print(Q.shape, K.shape, V.shape)\n",
        "        scoremats = torch.einsum(\n",
        "            \"BTH,BSH->BTS\", Q, K\n",
        "        )  # inner product of Q and K, a tensor\n",
        "        attnmats = F.softmax(\n",
        "            scoremats / np.sqrt(self.embed_dim), dim=-1\n",
        "        )  # softmax of scoremats\n",
        "        # print(scoremats.shape, attnmats.shape, )\n",
        "        ctx_vecs = torch.einsum(\n",
        "            \"BTS,BSH->BTH\", attnmats, V\n",
        "        )  # weighted average value vectors by attnmats\n",
        "        return ctx_vecs\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    \"\"\"The transformer block that combines self-attn, cross-attn and feed forward neural net\"\"\"\n",
        "\n",
        "    def __init__(self, hidden_dim, context_dim):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.attn_self = CrossAttention(\n",
        "            hidden_dim,\n",
        "            hidden_dim,\n",
        "        )\n",
        "        self.attn_cross = CrossAttention(hidden_dim, hidden_dim, context_dim)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(hidden_dim)\n",
        "        self.norm2 = nn.LayerNorm(hidden_dim)\n",
        "        self.norm3 = nn.LayerNorm(hidden_dim)\n",
        "        # implement a 2 layer MLP with K*hidden_dim hidden units, and nn.GeLU nonlinearity #######\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, 3 * hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(3 * hidden_dim, hidden_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, context=None):\n",
        "        # Notice the + x as residue connections\n",
        "        x = self.attn_self(self.norm1(x)) + x\n",
        "        # Notice the + x as residue connections\n",
        "        x = self.attn_cross(self.norm2(x), context=context) + x\n",
        "        # Notice the + x as residue connections\n",
        "        x = self.ffn(self.norm3(x)) + x\n",
        "        return x\n",
        "\n",
        "\n",
        "class SpatialTransformer(nn.Module):\n",
        "    def __init__(self, hidden_dim, context_dim):\n",
        "        super(SpatialTransformer, self).__init__()\n",
        "        self.transformer = TransformerBlock(hidden_dim, context_dim)\n",
        "\n",
        "    def forward(self, x, context=None):\n",
        "        b, c, h, w = x.shape\n",
        "        x_in = x\n",
        "        # Combine the spatial dimensions and move the channel dimen to the end\n",
        "        x = rearrange(x, \"b c h w->b (h w) c\")\n",
        "        # Apply the sequence transformer\n",
        "        x = self.transformer(x, context)\n",
        "        # Reverse the process\n",
        "        x = rearrange(x, \"b (h w) c -> b c h w\", h=h, w=w)\n",
        "        # Residue\n",
        "        return x + x_in"
      ],
      "metadata": {
        "id": "aJgmfwwm_80h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Diffusion model (Latent)\n"
      ],
      "metadata": {
        "id": "7_FmN_Evlf9c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Latent_UNet_Tranformer(nn.Module):\n",
        "    \"\"\"A time-dependent score-based model built upon U-Net architecture.\"\"\"\n",
        "\n",
        "    def __init__(self, channels=[4, 64, 128, 256], embed_dim=256, text_dim=256,\n",
        "                 nblocks=1, ndepth=1, nheads=1, prehead=\"same\"):\n",
        "        \"\"\"Initialize a time-dependent score-based network.\n",
        "\n",
        "        Args:\n",
        "          channels: The number of channels for feature maps of each resolution.\n",
        "          embed_dim: The dimensionality of Gaussian random feature embeddings.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # Gaussian random feature embedding layer for time\n",
        "        self.time_embed = nn.Sequential(\n",
        "            GaussianFourierProjection(embed_dim=embed_dim),\n",
        "            nn.Linear(embed_dim, embed_dim),\n",
        "        )\n",
        "        # Encoding layers where the resolution decreases\n",
        "        self.conv1  = nn.Conv2d(channels[0], channels[1], 3, stride=1, bias=False)\n",
        "        self.dense1 = Dense(embed_dim, channels[1])\n",
        "        self.gnorm1 = nn.GroupNorm(4, num_channels=channels[1])\n",
        "        self.conv2  = nn.Conv2d(channels[1], channels[2], 3, stride=2, bias=False)\n",
        "        self.dense2 = Dense(embed_dim, channels[2])\n",
        "        self.gnorm2 = nn.GroupNorm(4, num_channels=channels[2])\n",
        "        self.attn2  = HF_Encoder(nblocks, channels[2], text_dim, ndepth, nheads, prehead)\n",
        "        self.conv3  = nn.Conv2d(channels[2], channels[3], 3, stride=2, bias=False)\n",
        "        self.dense3 = Dense(embed_dim, channels[3])\n",
        "        self.gnorm3 = nn.GroupNorm(4, num_channels=channels[3])\n",
        "        self.attn3  = HF_Encoder(nblocks, channels[3], text_dim, ndepth, nheads, prehead)\n",
        "\n",
        "        self.tconv3 = nn.ConvTranspose2d(\n",
        "            channels[3], channels[2], 3, stride=2, bias=False,\n",
        "        )\n",
        "\n",
        "        self.dense6  = Dense(embed_dim, channels[2])\n",
        "        self.tgnorm3 = nn.GroupNorm(4, num_channels=channels[2])\n",
        "        self.attn6   = HF_Encoder(nblocks, channels[2], text_dim, ndepth, nheads, prehead)\n",
        "        self.tconv2  = nn.ConvTranspose2d(channels[2], channels[1], 3, stride=2, bias=False, output_padding=1\n",
        "        )  # + channels[2]\n",
        "        self.dense7  = Dense(embed_dim, channels[1])\n",
        "        self.tgnorm2 = nn.GroupNorm(4, num_channels=channels[1])\n",
        "        self.tconv1  = nn.ConvTranspose2d(channels[1], channels[0], 3, stride=1\n",
        "        )  # + channels[1]\n",
        "\n",
        "        # The swish activation function\n",
        "        self.act = nn.SiLU()  # lambda x: x * torch.sigmoid(x)\n",
        "        #self.cond_embed = nn.Embedding(nClass, text_dim)\n",
        "\n",
        "    def forward(self, x, t, y=None):\n",
        "        # Obtain the Gaussian random feature embedding for t\n",
        "        embed = self.act(self.time_embed(t))\n",
        "        y_embed = self.cond_embed(y).unsqueeze(1)\n",
        "        # Encoding path\n",
        "        ## Incorporate information from t\n",
        "        h1 = self.conv1(x) + self.dense1(embed)\n",
        "        ## Group normalization\n",
        "        h1 = self.act(self.gnorm1(h1))\n",
        "        h2 = self.conv2(h1) + self.dense2(embed)\n",
        "        h2 = self.act(self.gnorm2(h2))\n",
        "        h2 = self.attn2(h2)\n",
        "        h3 = self.conv3(h2) + self.dense3(embed)\n",
        "        h3 = self.act(self.gnorm3(h3))\n",
        "        h3 = self.attn3(h3)\n",
        "\n",
        "        # embdedded Residuals blocks representation? ###\n",
        "\n",
        "        # Decoding path\n",
        "        ## Skip connection from the encoding path\n",
        "        h = self.tconv3(h3) + self.dense6(embed)\n",
        "        h = self.act(self.tgnorm3(h))\n",
        "        h = self.attn6(h)\n",
        "        h = self.tconv2(h + h2)\n",
        "        h += self.dense7(embed)\n",
        "        h = self.act(self.tgnorm2(h))\n",
        "        h = self.tconv1(h + h1)\n",
        "\n",
        "        return h\n",
        "        # Check it out and test !!!!"
      ],
      "metadata": {
        "id": "KZcCn2GZRUAs"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iWqNQT7JRY2h"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
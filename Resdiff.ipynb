{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Latent diffusion attempt\n"
      ],
      "metadata": {
        "id": "wUigOX0zQOgx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SXg64As-QLzf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74bdeb30-b050-4807-eade-8cbc96d1003a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.8.0)\n",
            "Collecting lpips\n",
            "  Downloading lpips-0.1.4-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: torch>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from lpips) (2.5.0+cu121)\n",
            "Requirement already satisfied: torchvision>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from lpips) (0.20.0+cu121)\n",
            "Requirement already satisfied: numpy>=1.14.3 in /usr/local/lib/python3.10/dist-packages (from lpips) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from lpips) (1.13.1)\n",
            "Requirement already satisfied: tqdm>=4.28.1 in /usr/local/lib/python3.10/dist-packages (from lpips) (4.66.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->lpips) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=0.4.0->lpips) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.2.1->lpips) (10.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=0.4.0->lpips) (3.0.2)\n",
            "Downloading lpips-0.1.4-py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.8/53.8 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lpips\n",
            "Successfully installed lpips-0.1.4\n"
          ]
        }
      ],
      "source": [
        "!pip install einops -q\n",
        "!pip install lpips -q\n",
        "\n",
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from einops import rearrange\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import MNIST\n",
        "import tqdm\n",
        "from tqdm.notebook import trange, tqdm\n",
        "from torch.optim.lr_scheduler import MultiplicativeLR, LambdaLR\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lin1 = torch.nn.Linear(64, 32)\n",
        "lin2 = torch.nn.Linear(32, 16)\n",
        "x = torch.rand((16,4096, 64))\n",
        "lin2(lin1(x)).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sdCpgyojdI6B",
        "outputId": "e8b3b878-1654-4628-a824-5906e6b95943"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([16, 4096, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "General diffusion utils"
      ],
      "metadata": {
        "id": "g31DIcVx_yc2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "beta_min = 1e-4\n",
        "beta_max = 0.02\n",
        "\n",
        "num_steps = 500\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "betas = np.linspace(beta_min, beta_max, num_steps)\n",
        "betas = torch.from_numpy(betas).float().to(DEVICE)\n",
        "\n",
        "alphas = 1.0 - betas\n",
        "alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
        "alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0)\n",
        "\n",
        "sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n",
        "sqrt_one_minus_alphas_cumprod = torch.sqrt(1.0 - alphas_cumprod)\n",
        "log_one_minus_alphas_cumprod = torch.log(1.0 - alphas_cumprod)\n",
        "sqrt_recip_alphas_cumprod = torch.sqrt(1.0 / alphas_cumprod)\n",
        "sqrt_recipm1_alphas_cumprod = torch.sqrt(1.0 / alphas_cumprod - 1)\n",
        "\n",
        "posterior_variance = betas * (1.0 - alphas_cumprod_prev) / (1.0 - alphas_cumprod)\n",
        "posterior_log_variance_clipped = torch.log(posterior_variance.clamp(min=1e-20))\n",
        "posterior_mean_coef1 = betas * torch.sqrt(alphas_cumprod_prev) / (1.0 - alphas_cumprod)\n",
        "posterior_mean_coef2 = (\n",
        "    (1.0 - alphas_cumprod_prev) * torch.sqrt(alphas) / (1.0 - alphas_cumprod))"
      ],
      "metadata": {
        "id": "T-cLg-lu_x8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loss function definition:\n",
        "\n",
        "$ J := \\mathbb{E}_{t\\in (0, T), x_0 \\sim p_0(x_0), \\epsilon \\sim \\mathcal N(0,I)}\\left[ \\ \\Vert  \\epsilon - \\epsilon_{\\theta}(x_t, t) \\Vert^2  \\ \\right] $\n",
        "\n",
        "Considering $x_{residuals} = x_{HR}- up(x_{LR})$ as $x_t$"
      ],
      "metadata": {
        "id": "n9iJFJGEIGAx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_fn_cond(model, x, y):\n",
        "    \"\"\"The loss function for training score-based generative models.\n",
        "\n",
        "    Args:\n",
        "      model: A PyTorch model instance that represents a\n",
        "        time-dependent score-based model.\n",
        "      x: A mini-batch of LR training data.\n",
        "      y: Corresponding HR result.\n",
        "    \"\"\"\n",
        "    # Estimate Residuals between LR-HR\n",
        "    residuals = torch.subtract(x, y)\n",
        "\n",
        "    # Sample time uniformly in 0, num_steps\n",
        "    random_t = torch.randint(low=0, high=num_steps, size=(x.shape[0],), device=DEVICE)\n",
        "    z = torch.randn_like(x)  # get normally distributed noise\n",
        "\n",
        "    perturbed_x = (\n",
        "        sqrt_alphas_cumprod[random_t][..., None, None, None] * residuals \\\n",
        "        + sqrt_one_minus_alphas_cumprod[random_t][..., None, None, None] * z\n",
        "    )\n",
        "    eps = model(perturbed_x, random_t)\n",
        "    loss = torch.mean((z - eps) ** 2)\n",
        "    return loss"
      ],
      "metadata": {
        "id": "-WCPCJy6GlTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Defining Autoencoder\n"
      ],
      "metadata": {
        "id": "qs-yOEpoSc6n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AEmodel(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, input_channels, channels=[8,16,32]):\n",
        "        \"\"\"Args:\n",
        "          channels: The number of channels for feature maps of each resolution.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # Gaussian random feature embedding layer for time\n",
        "        # Encoding layers where the resolution decreases\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(input_channels, channels[0], 3, stride=1, bias=True),\n",
        "            nn.BatchNorm2d(channels[0]),\n",
        "            nn.SiLU(),\n",
        "            nn.Conv2d(channels[0], channels[1], 3, stride=2, bias=True),\n",
        "            nn.BatchNorm2d(channels[1]),\n",
        "            nn.SiLU(),\n",
        "            nn.Conv2d(channels[1], channels[2], 3, stride=1, bias=True),\n",
        "            nn.BatchNorm2d(channels[2]))\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(channels[2], channels[1], 3, stride=1, bias=True),\n",
        "            nn.BatchNorm2d(channels[1]),\n",
        "            nn.SiLU(),\n",
        "            nn.ConvTranspose2d(channels[1], channels[0], 3, stride=2, bias=True, output_padding=1),\n",
        "            nn.BatchNorm2d(channels[0]),\n",
        "            nn.SiLU(),\n",
        "            nn.ConvTranspose2d(channels[0], input_channels, 3, stride=1, bias=True),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.decoder(self.encoder(x))"
      ],
      "metadata": {
        "id": "i-WfhmmsQcen"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Autoencoder with time embdedding"
      ],
      "metadata": {
        "id": "E4WdWUfvSgaO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining time embedding. (not actually clear why we may need a time embdedded autoencoder)"
      ],
      "metadata": {
        "id": "ZaWO6jjBStlz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GaussianFourierProjection(nn.Module):\n",
        "    \"\"\"Gaussian random features for encoding time steps.\"\"\"\n",
        "\n",
        "    def __init__(self, embed_dim, scale=30.0):\n",
        "        super().__init__()\n",
        "        # Randomly sample weights (frequencies) during initialization.\n",
        "        # These weights (frequencies) are fixed during optimization and are not trainable.\n",
        "        self.W = nn.Parameter(torch.randn(embed_dim // 2) * scale, requires_grad=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\" Args:\n",
        "            x: torch.Tensor of dim 1 (e.g. torch.Tensor([12]))\n",
        "        \"\"\"\n",
        "        x_proj = x[:, None] * self.W[None, :] * 2 * np.pi\n",
        "        return torch.cat([torch.sin(x_proj), torch.cos(x_proj)], dim=-1)\n",
        "\n",
        "\n",
        "class Dense(nn.Module):\n",
        "    \"\"\"A fully connected layer that reshapes outputs to feature maps.\n",
        "    Allow time repr to input additively from the side of a convolution layer.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.dense(x)[..., None, None]\n",
        "        # this broadcast the 2d tensor to 4d, add the same value across space.\n",
        "\n",
        "\n",
        "class AEmodel(nn.Module):\n",
        "\n",
        "    def __init__(self, input_channels, channels=[4, 8, 16], embed_dim=256):\n",
        "        \"\"\"Args:\n",
        "          channels: The number of channels for feature maps of each resolution.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        pass\n",
        "\n",
        "    def forward(self, x):\n",
        "        raise NotImplementedError\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_u5rfiezSt3b",
        "outputId": "a8bc4785-a4d8-4e54-f7fd-0de6cb36cb23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedded dim: torch.Size([1, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Autoencoder Training\n",
        "Decide whether to have a pretrained autoencoder for class domain (climate variable)\n"
      ],
      "metadata": {
        "id": "NYw44tdDlc7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "se = SimpleEncoder(1)\n",
        "x = torch.rand((4,1,32,32))\n",
        "pytorch_total_params = sum(p.numel() for p in se.parameters() if p.requires_grad)\n",
        "print(se(x).shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LeydnHNKiTz",
        "outputId": "651bd086-1694-4b21-aa0b-14997d5ad842"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 32, 8, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Attention modules\n"
      ],
      "metadata": {
        "id": "fDPp9VXvK5Oo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Channel Cross-Attention approach\n",
        "Requires a joint latent representation including the climate data along with its high frequency representation to match H·W dimension at each downsampling stage.\n",
        "\n"
      ],
      "metadata": {
        "id": "OjZSKId5ryZ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResImgEncoder(nn.Module):\n",
        "    \"\"\"Simple Image encoder to be used in the Q, K, V transcription\"\"\"\n",
        "    def __init__(self, nchannels, ndepth=1, nheads=2, prehead=True):\n",
        "        \"\"\" Args:\n",
        "            nchannels: (int) input x_feature channel\n",
        "            ndepth: (int)\n",
        "            nheads: (int) (set to 1 to single-head if you want to preserve\n",
        "             in_dim == out_dim)\n",
        "            prehead: (bool)\n",
        "        \"\"\"\n",
        "        super(ResImgEncoder, self).__init__()\n",
        "        self.outchannels = nchannels * nheads\n",
        "        self.ndepth = ndepth\n",
        "\n",
        "        if prehead:\n",
        "            # move first into outchannel dim via 1x1 convblock and go on with\n",
        "            # ResBlocks at outchannels\n",
        "            in_c, mid_c, out_c = nchannels, self.outchannels, self.outchannels\n",
        "        else:\n",
        "            # enlarge channel dimension at the end\n",
        "            in_c, mid_c, out_c = nchannels, nchannels, self.outchannels\n",
        "\n",
        "        self.conv1pre  = nn.Conv2d(in_c, mid_c, 1, stride=1, bias=True)\n",
        "        self.res_block = nn.Sequential(\n",
        "            nn.Conv2d(mid_c, mid_c, 3, stride=1, padding=\"same\", bias=True),\n",
        "            nn.BatchNorm2d(mid_c),\n",
        "            nn.SiLU(),\n",
        "            nn.Conv2d(mid_c, mid_c, 3, stride=1, padding=\"same\", bias=True),\n",
        "            nn.BatchNorm2d(mid_c),\n",
        "            )\n",
        "        self.conv1post = nn.Conv2d(mid_c, out_c, 1, stride=1, bias=True)\n",
        "        self.act = nn.SiLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1pre(x)\n",
        "        for _ in range(self.ndepth):\n",
        "            x = self.act(self.res_block(x) + x)\n",
        "        x = self.conv1post(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class CrossAttention(nn.Module):\n",
        "    \"\"\"Simple Cross-attention High Frequency Guided Module\"\"\"\n",
        "    def __init__(self, in_channel, ndepth=1, nheads=2, prehead=True):\n",
        "        super(CrossAttention, self).__init__()\n",
        "        # Learnable d parameter\n",
        "        self.d = torch.nn.Parameter(\n",
        "                torch.abs(torch.rand((1)) + torch.randint(10//2, 10*2, (1,))\n",
        "                ))\n",
        "        self.d.requires_grad=True\n",
        "        # Convolutional encoder to process Q, K, V\n",
        "        self.feature_encoder   = ResImgEncoder(in_channel, ndepth, nheads, prehead)\n",
        "        self.crossattn_encoder = ResImgEncoder(1, ndepth, nheads=1) ### Go on from here\n",
        "        self.conv3 = nn.Conv2d(1, 1, kernel_size=3, stride=1, padding=\"same\", bias=False)\n",
        "        self.conv1 = nn.Conv2d(in_channel, in_channel, kernel_size=1, stride=1, bias=False)\n",
        "\n",
        "# !!! you need to convert the dimension back to the original (modify the conv1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\" Args:\n",
        "        x: (torch.Tensor) LR image of dim (b, c, h, w)\n",
        "        \"\"\"\n",
        "        b, c, h, w = x.shape\n",
        "        # tokens: with shape (batch, h*w, c)\n",
        "        Q = self.feature_encoder(x).view(b, h*w, c)\n",
        "        K = Q.clone()\n",
        "        V = Q.clone()\n",
        "        #scoremats = torch.einsum(\"BTH,BSH->BTS\", Q, K)  # inner product of Q and K, a tensor\n",
        "        # Channel-wise product\n",
        "        scoremats = torch.einsum(\"bjk,bjc->bkc\", Q, K).view(b, 1, c, c)\n",
        "        scoremats = self.crossattn_encoder(scoremats).view(b, c, c)\n",
        "        attnmats  = F.softmax(scoremats / torch.sqrt(self.d), dim=-1)\n",
        "        #ctx_vecs  = torch.einsum(\"BTS,BSH->BTH\", attnmats, V)\n",
        "        # To (batch, hdim, h, w)\n",
        "        ctx_vecs  = torch.einsum(\"bkc,bij->bic\", attnmats, V).view(b, c, h, w)\n",
        "        # 1k conv to (batch, in_channel, h , w)\n",
        "        ctx_vecs  = self.conv1(ctx_vecs)\n",
        "        return x + ctx_vecs\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    pass\n",
        "    # concatenate multiple spatial attention (CrossAtt blocks)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import time\n",
        "    t0=time.time()\n",
        "    nchannels = 1 1\n",
        "    x = torch.rand((1, nchannels, 64, 64))\n",
        "    ca = CrossAttention(nchannels, ndepth=1)\n",
        "    t1 = time.time()\n",
        "    print(ca(x).shape, t1-t0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "f2nPd76sryqJ",
        "outputId": "8ce3bf4e-1748-46d0-b852-95e4ce6b4f56"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-51-e49de98cd51d>, line 61)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-51-e49de98cd51d>\"\u001b[0;36m, line \u001b[0;32m61\u001b[0m\n\u001b[0;31m    nchannels = 1 1\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sequence based attention implementation"
      ],
      "metadata": {
        "id": "dC6dEXoQUo3L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ImgEncoder(nn.Module):\n",
        "    \"\"\"Simple Image encoder to be used in the Q, K, V transcription\"\"\"\n",
        "    def __init__(self, input_channels, channels=[8,16,32]):\n",
        "        super(ImgEncoder, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(input_channels, channels[0], 3, stride=1, padding=\"same\", bias=True),\n",
        "            nn.BatchNorm2d(channels[0]),\n",
        "            nn.MaxPool2d(2, stride=2),\n",
        "            nn.SiLU(),\n",
        "            nn.Conv2d(channels[0], channels[1], 3, stride=1, padding=\"same\", bias=True),\n",
        "            nn.BatchNorm2d(channels[1]),\n",
        "            nn.MaxPool2d(2, stride=2),\n",
        "            nn.SiLU(),\n",
        "            nn.Conv2d(channels[1], channels[2], 3, stride=1, padding=\"same\", bias=True),\n",
        "            nn.BatchNorm2d(channels[2])\n",
        "            )\n",
        "    def forward(self, x):\n",
        "        \"\"\" outdim: [b, channels[-1], H/4, W/4] \"\"\"\n",
        "        x = self.encoder(x)\n",
        "        return x # reduced H_{out} = H/4\n",
        "\n",
        "\n",
        "class CrossAttention(nn.Module):\n",
        "    \"\"\"Simple Cross-attention High Frequency Guided Module\"\"\"\n",
        "    def __init__(self, in_channel, feature_dim, hidden_dim=256, out_dim=256):\n",
        "        super(CrossAttention, self).__init__()\n",
        "        self.hidden_dim  = hidden_dim\n",
        "        self.h, self.w = feature_dim\n",
        "\n",
        "        # Learnable d parameter\n",
        "        self.d = torch.nn.Parameter(\n",
        "                torch.abs(torch.rand((1)) + torch.randint(10//2, 10*2, (1,))\n",
        "                ))\n",
        "        self.d.requires_grad=True\n",
        "\n",
        "        # Convolutional encoder to process Q, K, V\n",
        "        self.HF_encoder   = SimpleEncoder(in_channel)\n",
        "        self.feat_encoder = SimpleEncoder(in_channel)\n",
        "        self.linear = nn.Sequential(\n",
        "            nn.Linear(self.h//4*self.w//4, hidden_dim, bias=True),\n",
        "            nn.GELU,\n",
        "            nn.Linear(hidden_dim, out_dim)\n",
        "            )\n",
        "\n",
        "    def forward(self, x, orig_hf):\n",
        "        \"\"\" Args:\n",
        "        x: (torch.Tensor) LR image of dim (b, c, h, w)\n",
        "        orig_hf: (torch.Tensor) original High-Frequency map of dim (b, 1, h, w)\n",
        "        \"\"\"\n",
        "        b, _, h, w = x.shape\n",
        "        # tokens: with shape (batch, h*w, hdim)\n",
        "        Q = self.convHF(orig_hf).view(b, h*w, self.hidden_dim)\n",
        "        Q = self.linear(Q).view(b, )\n",
        "\n",
        "        K = self.convK(x).view(b, h*w, self.hidden_dim)\n",
        "        V = self.convV(x).view(b, h*w, self.hidden_dim)\n",
        "\n",
        "        #scoremats = torch.einsum(\"BTH,BSH->BTS\", Q, K)  # inner product of Q and K, a tensor\n",
        "        # Channel-wise product\n",
        "        scoremats = torch.einsum(\"bjk,bjc->bkc\", Q, K)\\\n",
        "                    .view(b, 1, self.hidden_dim, self.hidden_dim)\n",
        "        scoremats = self.convA(scoremats)\\\n",
        "                    .view(b, self.hidden_dim, self.hidden_dim)\n",
        "        attnmats  = F.softmax(scoremats / torch.sqrt(self.d), dim=-1)\n",
        "        #ctx_vecs  = torch.einsum(\"BTS,BSH->BTH\", attnmats, V)\n",
        "        # To (batch, hdim, h, w)\n",
        "        ctx_vecs  = torch.einsum(\"bkc,bij->bic\", attnmats, V)\\\n",
        "                    .view(b, self.hidden_dim, h, w)\n",
        "        # 1k conv to (batch, in_channel, h , w)\n",
        "        ctx_vecs  = self.conv1(ctx_vecs)\n",
        "        return x + ctx_vecs\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    \"\"\"The transformer block that combines self-attn, cross-attn and feed forward neural net\"\"\"\n",
        "\n",
        "    def __init__(self, in_channel, hidden_dim):# context_dim):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.attn_self = CrossAttention(in_channel, hidden_dim)\n",
        "        #self.attn_cross = CrossAttention(hidden_dim, hidden_dim, context_dim)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(hidden_dim)\n",
        "        self.norm2 = nn.LayerNorm(hidden_dim)\n",
        "        self.norm3 = nn.LayerNorm(hidden_dim)\n",
        "        # implement a 2 layer MLP with K*hidden_dim hidden units, and nn.GeLU nonlinearity #######\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, 3 * hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(3 * hidden_dim, hidden_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, orig_hf):#, context=None):\n",
        "        \"\"\" Args:\n",
        "        x: (torch.Tensor) LR image of dim (b, c, h, w)\n",
        "        orig_hf: (torch.Tensor) original High-Frequency map of dim (b, 1, h, w)\n",
        "        \"\"\"\n",
        "        # Notice the + x as residue connections\n",
        "        x = self.attn_self(self.norm1(x), orig_hf) + x\n",
        "        # Notice the + x as residue connections\n",
        "        #x = self.attn_cross(self.norm2(x), context=context) + x\n",
        "        # Notice the + x as residue connections\n",
        "        x = self.ffn(self.norm3(x)) + x\n",
        "        return x\n",
        "\n",
        "\n",
        "class SpatialTransformer(nn.Module):\n",
        "    def __init__(self, in_channel, hidden_dim):\n",
        "        super(SpatialTransformer, self).__init__()\n",
        "        self.hf_encoder  = ImgEncoder(1, channels=[8,16,32])\n",
        "        self.img_encoder = ImgEncoder(in_channel, channels=[8,16,32])\n",
        "\n",
        "        self.transformer = TransformerBlock(in_channel, hidden_dim)\n",
        "\n",
        "    def forward(self, x, orig_hf):# context=None):\n",
        "        b, c, h, w = x.shape\n",
        "        x_in = x\n",
        "        # Combine the spatial dimensions and move the channel dimen to the end\n",
        "        x = rearrange(x, \"b c h w->b (h w) c\")\n",
        "        # Apply the sequence transformer\n",
        "        x = self.transformer(x, orig_hf)\n",
        "        # Reverse the process\n",
        "        x = rearrange(x, \"b (h w) c -> b c h w\", h=h, w=w)\n",
        "        # Residue\n",
        "        return x + x_in"
      ],
      "metadata": {
        "id": "gbtdYm93SaT0"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fully working Cross Attention mechanism with images"
      ],
      "metadata": {
        "id": "JFsGr2M1UlBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CrossAttention(nn.Module):\n",
        "    \"\"\"Simple Cross-attention High Frequency Guided Module\"\"\"\n",
        "    def __init__(self, in_channel, hidden_dim=128):\n",
        "        super(CrossAttention, self).__init__()\n",
        "        self.hidden_dim  = hidden_dim\n",
        "        # Learnable d parameter\n",
        "        self.d = torch.nn.Parameter(\n",
        "                torch.abs(torch.rand((1)) + torch.randint(10//2, 10*2, (1,))\n",
        "                ))\n",
        "        self.d.requires_grad=True\n",
        "\n",
        "        # Convolutional encoder to process Q, K, V\n",
        "        self.convHF = nn.Conv2d(1, hidden_dim, 3, 1, \"same\", bias=False)\n",
        "        self.convK  = nn.Conv2d(in_channel, hidden_dim, 3, 1, \"same\", bias=False)\n",
        "        self.convV  = nn.Conv2d(in_channel, hidden_dim, 3, 1, \"same\", bias=False)\n",
        "        self.convA  = nn.Conv2d(1, 1, kernel_size=3, stride=1, padding=\"same\", bias=False)\n",
        "        self.conv1  = nn.Conv2d(hidden_dim, in_channel, 1, bias=False)\n",
        "\n",
        "    def forward(self, x, orig_hf):\n",
        "        \"\"\" Args:\n",
        "        x: (torch.Tensor) LR image of dim (b, c, h, w)\n",
        "        orig_hf: (torch.Tensor) original High-Frequency map of dim (b, 1, h, w)\n",
        "        \"\"\"\n",
        "        b, _, h, w = x.shape\n",
        "        # tokens: with shape (batch, h*w, hdim)\n",
        "        Q = self.convHF(orig_hf).view(b, h*w, self.hidden_dim)\n",
        "        K = self.convK(x).view(b, h*w, self.hidden_dim)\n",
        "        V = self.convV(x).view(b, h*w, self.hidden_dim)\n",
        "\n",
        "\n",
        "        #scoremats = torch.einsum(\"BTH,BSH->BTS\", Q, K)  # inner product of Q and K, a tensor\n",
        "        # Channel-wise product\n",
        "        scoremats = torch.einsum(\"bjk,bjc->bkc\", Q, K)\\\n",
        "                    .view(b, 1, self.hidden_dim, self.hidden_dim)\n",
        "        scoremats = self.convA(scoremats)\\\n",
        "                    .view(b, self.hidden_dim, self.hidden_dim)\n",
        "        attnmats  = F.softmax(scoremats / torch.sqrt(self.d), dim=-1)\n",
        "        #ctx_vecs  = torch.einsum(\"BTS,BSH->BTH\", attnmats, V)\n",
        "        # To (batch, hdim, h, w)\n",
        "        ctx_vecs  = torch.einsum(\"bkc,bij->bic\", attnmats, V)\\\n",
        "                    .view(b, self.hidden_dim, h, w)\n",
        "        # 1k conv to (batch, in_channel, h , w)\n",
        "        ctx_vecs  = self.conv1(ctx_vecs)\n",
        "        return x + ctx_vecs\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    \"\"\"The transformer block that combines self-attn, cross-attn and feed forward neural net\"\"\"\n",
        "\n",
        "    def __init__(self, in_channel, hidden_dim):# context_dim):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.attn_self = CrossAttention(in_channel, hidden_dim)\n",
        "        #self.attn_cross = CrossAttention(hidden_dim, hidden_dim, context_dim)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(hidden_dim)\n",
        "        self.norm2 = nn.LayerNorm(hidden_dim)\n",
        "        self.norm3 = nn.LayerNorm(hidden_dim)\n",
        "        # implement a 2 layer MLP with K*hidden_dim hidden units, and nn.GeLU nonlinearity #######\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, 3 * hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(3 * hidden_dim, hidden_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, orig_hf):#, context=None):\n",
        "        \"\"\" Args:\n",
        "        x: (torch.Tensor) LR image of dim (b, c, h, w)\n",
        "        orig_hf: (torch.Tensor) original High-Frequency map of dim (b, 1, h, w)\n",
        "        \"\"\"\n",
        "        # Notice the + x as residue connections\n",
        "        x = self.attn_self(self.norm1(x), orig_hf) + x\n",
        "        # Notice the + x as residue connections\n",
        "        #x = self.attn_cross(self.norm2(x), context=context) + x\n",
        "        # Notice the + x as residue connections\n",
        "        x = self.ffn(self.norm3(x)) + x\n",
        "        return x\n",
        "\n",
        "\n",
        "class SpatialTransformer(nn.Module):\n",
        "    def __init__(self, in_channel, hidden_dim):\n",
        "        super(SpatialTransformer, self).__init__()\n",
        "        self.transformer = TransformerBlock(in_channel, hidden_dim)\n",
        "\n",
        "    def forward(self, x, orig_hf):# context=None):\n",
        "        b, c, h, w = x.shape\n",
        "        x_in = x\n",
        "        # Combine the spatial dimensions and move the channel dimen to the end\n",
        "        x = rearrange(x, \"b c h w->b (h w) c\")\n",
        "        # Apply the sequence transformer\n",
        "        x = self.transformer(x, orig_hf)\n",
        "        # Reverse the process\n",
        "        x = rearrange(x, \"b (h w) c -> b c h w\", h=h, w=w)\n",
        "        # Residue\n",
        "        return x + x_in\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "    in_channel = 1\n",
        "    hidden_dim = 64\n",
        "    ca = CrossAttention(in_channel, hidden_dim)\n",
        "    x = torch.rand((4, in_channel, 64, 64))\n",
        "    orig_hf = torch.rand((4, 1, 64, 64))\n",
        "\n",
        "    out = ca(x,orig_hf)\n",
        "    conv1 = nn.Conv2d(hidden_dim, 1, 1, bias=False)\n",
        "\n",
        "    #tb = TransformerBlock(in_channel, hidden_dim)\n",
        "    #norm = nn.LayerNorm(hidden_dim)\n",
        "    #print(norm(x).shape)\n",
        "    print(\"CrossAtt output\", out.shape)\n",
        "    #print(\"Transfb output:\", tb(x, orig_hf).shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zoztwVc2K7Rw",
        "outputId": "f10008e1-331a-44f7-f0ec-c591de7cf9a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CrossAtt output torch.Size([4, 1, 64, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Orig implementation"
      ],
      "metadata": {
        "id": "pJvPXslrU3jG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CrossAttention(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        embed_dim,\n",
        "        hidden_dim,\n",
        "        context_dim=None,\n",
        "        num_heads=1,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Note: For simplicity reason, we just implemented 1-head attention.\n",
        "        Feel free to implement multi-head attention! with fancy tensor manipulations.\n",
        "        \"\"\"\n",
        "        super(CrossAttention, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.context_dim = context_dim\n",
        "        self.embed_dim = embed_dim\n",
        "        self.query = nn.Linear(hidden_dim, embed_dim, bias=False)\n",
        "        if context_dim is None:\n",
        "            self.self_attn = True\n",
        "            self.key = nn.Linear(hidden_dim, embed_dim, bias=False)  ###########\n",
        "            self.value = nn.Linear(hidden_dim, hidden_dim, bias=False)  ############\n",
        "        else:\n",
        "            self.self_attn = False\n",
        "            self.key = nn.Linear(context_dim, embed_dim, bias=False)  #############\n",
        "            self.value = nn.Linear(context_dim, hidden_dim, bias=False)  ############\n",
        "\n",
        "    def forward(self, tokens, context=None):\n",
        "        # tokens: with shape [batch, sequence_len, hidden_dim]\n",
        "        # context: with shape [batch, contex_seq_len, context_dim]\n",
        "        if self.self_attn:\n",
        "            Q = self.query(tokens)\n",
        "            K = self.key(tokens)\n",
        "            V = self.value(tokens)\n",
        "        else:\n",
        "            # implement Q, K, V for the Cross attention\n",
        "            Q = self.query(tokens)\n",
        "            K = self.key(context)\n",
        "            V = self.value(context)\n",
        "        # print(Q.shape, K.shape, V.shape)\n",
        "        scoremats = torch.einsum(\n",
        "            \"BTH,BSH->BTS\", Q, K\n",
        "        )  # inner product of Q and K, a tensor\n",
        "        attnmats = F.softmax(\n",
        "            scoremats / np.sqrt(self.embed_dim), dim=-1\n",
        "        )  # softmax of scoremats\n",
        "        # print(scoremats.shape, attnmats.shape, )\n",
        "        ctx_vecs = torch.einsum(\n",
        "            \"BTS,BSH->BTH\", attnmats, V\n",
        "        )  # weighted average value vectors by attnmats\n",
        "        return ctx_vecs\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    \"\"\"The transformer block that combines self-attn, cross-attn and feed forward neural net\"\"\"\n",
        "\n",
        "    def __init__(self, hidden_dim, context_dim):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.attn_self = CrossAttention(\n",
        "            hidden_dim,\n",
        "            hidden_dim,\n",
        "        )\n",
        "        self.attn_cross = CrossAttention(hidden_dim, hidden_dim, context_dim)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(hidden_dim)\n",
        "        self.norm2 = nn.LayerNorm(hidden_dim)\n",
        "        self.norm3 = nn.LayerNorm(hidden_dim)\n",
        "        # implement a 2 layer MLP with K*hidden_dim hidden units, and nn.GeLU nonlinearity #######\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, 3 * hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(3 * hidden_dim, hidden_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, context=None):\n",
        "        # Notice the + x as residue connections\n",
        "        x = self.attn_self(self.norm1(x)) + x\n",
        "        # Notice the + x as residue connections\n",
        "        x = self.attn_cross(self.norm2(x), context=context) + x\n",
        "        # Notice the + x as residue connections\n",
        "        x = self.ffn(self.norm3(x)) + x\n",
        "        return x\n",
        "\n",
        "\n",
        "class SpatialTransformer(nn.Module):\n",
        "    def __init__(self, hidden_dim, context_dim):\n",
        "        super(SpatialTransformer, self).__init__()\n",
        "        self.transformer = TransformerBlock(hidden_dim, context_dim)\n",
        "\n",
        "    def forward(self, x, context=None):\n",
        "        b, c, h, w = x.shape\n",
        "        x_in = x\n",
        "        # Combine the spatial dimensions and move the channel dimen to the end\n",
        "        x = rearrange(x, \"b c h w->b (h w) c\")\n",
        "        # Apply the sequence transformer\n",
        "        x = self.transformer(x, context)\n",
        "        # Reverse the process\n",
        "        x = rearrange(x, \"b (h w) c -> b c h w\", h=h, w=w)\n",
        "        # Residue\n",
        "        return x + x_in"
      ],
      "metadata": {
        "id": "aJgmfwwm_80h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Diffusion model (Latent)\n"
      ],
      "metadata": {
        "id": "7_FmN_Evlf9c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Latent_UNet_Tranformer(nn.Module):\n",
        "    \"\"\"A time-dependent score-based model built upon U-Net architecture.\"\"\"\n",
        "\n",
        "    def __init__(self, channels=[4, 64, 128, 256], embed_dim=256, text_dim=256):\n",
        "        \"\"\"Initialize a time-dependent score-based network.\n",
        "\n",
        "        Args:\n",
        "          channels: The number of channels for feature maps of each resolution.\n",
        "          embed_dim: The dimensionality of Gaussian random feature embeddings.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # Gaussian random feature embedding layer for time\n",
        "        self.time_embed = nn.Sequential(\n",
        "            GaussianFourierProjection(embed_dim=embed_dim),\n",
        "            nn.Linear(embed_dim, embed_dim),\n",
        "        )\n",
        "        # Encoding layers where the resolution decreases\n",
        "        self.conv1  = nn.Conv2d(channels[0], channels[1], 3, stride=1, bias=False)\n",
        "        self.dense1 = Dense(embed_dim, channels[1])\n",
        "        self.gnorm1 = nn.GroupNorm(4, num_channels=channels[1])\n",
        "        self.conv2  = nn.Conv2d(channels[1], channels[2], 3, stride=2, bias=False)\n",
        "        self.dense2 = Dense(embed_dim, channels[2])\n",
        "        self.gnorm2 = nn.GroupNorm(4, num_channels=channels[2])\n",
        "        self.attn2  = SpatialTransformer(channels[2], text_dim)\n",
        "        self.conv3  = nn.Conv2d(channels[2], channels[3], 3, stride=2, bias=False)\n",
        "        self.dense3 = Dense(embed_dim, channels[3])\n",
        "        self.gnorm3 = nn.GroupNorm(4, num_channels=channels[3])\n",
        "        self.attn3  = SpatialTransformer(channels[3], text_dim)\n",
        "\n",
        "        self.tconv3 = nn.ConvTranspose2d(\n",
        "            channels[3], channels[2], 3, stride=2, bias=False,\n",
        "        )\n",
        "\n",
        "        self.dense6  = Dense(embed_dim, channels[2])\n",
        "        self.tgnorm3 = nn.GroupNorm(4, num_channels=channels[2])\n",
        "        self.attn6   = SpatialTransformer(channels[2], text_dim)\n",
        "        self.tconv2  = nn.ConvTranspose2d(channels[2], channels[1], 3, stride=2, bias=False, output_padding=1\n",
        "        )  # + channels[2]\n",
        "        self.dense7  = Dense(embed_dim, channels[1])\n",
        "        self.tgnorm2 = nn.GroupNorm(4, num_channels=channels[1])\n",
        "        self.tconv1  = nn.ConvTranspose2d(channels[1], channels[0], 3, stride=1\n",
        "        )  # + channels[1]\n",
        "\n",
        "        # The swish activation function\n",
        "        self.act = nn.SiLU()  # lambda x: x * torch.sigmoid(x)\n",
        "        #self.cond_embed = nn.Embedding(nClass, text_dim)\n",
        "\n",
        "    def forward(self, x, t, y=None):\n",
        "        # Obtain the Gaussian random feature embedding for t\n",
        "        embed = self.act(self.time_embed(t))\n",
        "        y_embed = self.cond_embed(y).unsqueeze(1)\n",
        "        # Encoding path\n",
        "        ## Incorporate information from t\n",
        "        h1 = self.conv1(x) + self.dense1(embed)\n",
        "        ## Group normalization\n",
        "        h1 = self.act(self.gnorm1(h1))\n",
        "        h2 = self.conv2(h1) + self.dense2(embed)\n",
        "        h2 = self.act(self.gnorm2(h2))\n",
        "        h2 = self.attn2(h2, y_embed)\n",
        "        h3 = self.conv3(h2) + self.dense3(embed)\n",
        "        h3 = self.act(self.gnorm3(h3))\n",
        "        h3 = self.attn3(h3, y_embed)\n",
        "\n",
        "        # embdedded Residuals blocks representation? ###\n",
        "\n",
        "        # Decoding path\n",
        "        ## Skip connection from the encoding path\n",
        "        h = self.tconv3(h3) + self.dense6(embed)\n",
        "        h = self.act(self.tgnorm3(h))\n",
        "        h = self.attn6(h, y_embed)\n",
        "        h = self.tconv2(h + h2)\n",
        "        h += self.dense7(embed)\n",
        "        h = self.act(self.tgnorm2(h))\n",
        "        h = self.tconv1(h + h1)\n",
        "\n",
        "        return h"
      ],
      "metadata": {
        "id": "KZcCn2GZRUAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iWqNQT7JRY2h"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}